{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сбор характеристик недвижимости через CianParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cianparser\n",
        "\n",
        "spisochek = [\"Верея\", \"Высоковск\", \"Дрезна\", \"Талдом\", \"Руза\", \"Краснозаводск\", \n",
        "\"Пересвет\", \"Яхрома\", \"Голицыно\", \"Волоколамск\", \"Рошаль\", \"Кубинка\", \n",
        "\"Куровское\", \"Пущино\", \"Электроугли\", \"Черноголовка\", \"Хотьково\", \n",
        "\"Звенигород\", \"Бронницы\", \"Электрогорск\", \"Зарайск\", \"Старая Купавна\", \n",
        "\"Озёры\", \"Лосино-Петровский\", \"Красноармейск\", \"Ликино-Дулёво\", \"Можайск\", \n",
        "\"Луховицы\", \"Дедовск\", \"Апрелевка\", \"Шатура\", \"Истра\", \"Протвино\", \n",
        "\"Краснознаменск\", \"Кашира\", \"Котельники\", \"Солнечногорск\", \"Дзержинский\", \n",
        "\"Лыткарино\", \"Фрязино\", \"Павловский Посад\", \"Наро-Фоминск\", \"Ступино\", \n",
        "\"Дмитров\", \"Чехов\", \"Егорьевск\", \"Дубна\", \"Видное\", \"Клин\", \"Ивантеевка\", \n",
        "\"Лобня\", \"Воскресенск\", \"Сергиев Посад\", \"Ногинск\", \"Жуковский\", \"Пушкино\", \n",
        "\"Реутов\", \"Долгопрудный\", \"Орехово-Зуево\", \"Раменское\", \"Щёлково\", \n",
        "\"Серпухов\", \"Одинцово\", \"Домодедово\", \"Коломна\", \"Электросталь\", \n",
        "\"Красногорск\", \"Люберцы\", \"Королёв\", \"Мытищи\", \"Химки\", \"Подольск\", \"Балашиха\",\n",
        "\"Москва\"]\n",
        "\n",
        "# Сбор недвижимости по каждому из количества комнат\n",
        "for ii in spisochek:\n",
        "    pars = cianparser.CianParser(location=ii)\n",
        "    for i in range(1, 6):\n",
        "        data = pars.get_flats(\n",
        "            deal_type=\"sale\",\n",
        "            with_extra_data = True,\n",
        "            rooms=(i), \n",
        "            with_saving_csv=True, \n",
        "            additional_settings={\n",
        "                \"start_page\": 1, \n",
        "                \"end_page\": 54\n",
        "            })\n",
        "\n",
        "        print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Папка с CSV файлами\n",
        "path = \"C:/Users/User/Desktop/parser/\"\n",
        "filenames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "# Пустой датафрейм для хранения данных\n",
        "dfs = []\n",
        "\n",
        "# Перебираем все файлы в папке\n",
        "for filename in filenames:\n",
        "    df = pd.read_csv(filename, usecols=[0], error_bad_lines=False)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Объединение\n",
        "combined_data = pd.concat(dfs)\n",
        "\n",
        "# Сохранение объединенного датасета в новый файл b_data.csv\n",
        "combined_data.to_csv('b_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сбор дополнительных значений недвижимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "# файл для хранения информации\n",
        "rows_f = open('info.txt', 'a')\n",
        "rows_f.write(\"url,living_meters,kitchen_meters,y_c\\n\")\n",
        "\n",
        "err = open('errors.txt', 'a')  # файл хранящий возникающие ошибки\n",
        "success = 0\n",
        "\n",
        "# Параметры для requests.get()\n",
        "st_accept = 'text/html'\n",
        "st_useragent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
        "TIMEOUT = 25\n",
        "headers = {\n",
        "    \"Accept\": st_accept,\n",
        "    \"User-Agent\": st_useragent\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_number(s):\n",
        "    s = s.replace(\",\", \".\")  # замена запятой на точку для float\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return float(s)\n",
        "        except ValueError:\n",
        "            return s\n",
        "        \n",
        "\n",
        "def remove_non_numeric_values(lst):\n",
        "    return [x for x in lst if isinstance(x, (int, float))] # удаление нечисловых значений\n",
        "\n",
        "\n",
        "def get_info_from_link(url_value): # получение характеристик по ссылке\n",
        "    global b, success \n",
        "    \n",
        "    # Отправка запроса \n",
        "    time.sleep(7)\n",
        "    response = requests.get(url_value, headers=headers, timeout=TIMEOUT)\n",
        "    print(response)\n",
        "    if response.status_code == 200:\n",
        "        success += 1\n",
        "\n",
        "    html = response.content\n",
        "\n",
        "    # Анализ контекнта с помощью BeautifulSoup\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Поиск тегов с классом\n",
        "    p_tags = soup.find_all('p', class_='a10a3f92e9--color_black_100--Ephi7')\n",
        "\n",
        "    # Извлечение характеристик\n",
        "    characteristics = []\n",
        "    for p_tag in p_tags:\n",
        "        characteristic = p_tag.text.strip()  # извлечение текста\n",
        "        characteristics.append(characteristic)  # добавление в список\n",
        "\n",
        "    # удаление нечисловых значений\n",
        "    characteristics = remove_non_numeric_values([convert_to_number(c) for c in characteristics])\n",
        "\n",
        "\n",
        "    # удаление значений после даты постройки\n",
        "    for i, item in enumerate(characteristics):\n",
        "        if isinstance(item, int) and 1000 <= item <= 9999:\n",
        "            del characteristics[i+1:]\n",
        "            break\n",
        "\n",
        "    # поиск даты постройки с добвлением в столбец year_of_construction\n",
        "    y_c = next((item for item in characteristics if isinstance(item, int) and 1000 <= item <= 3000), 0)\n",
        "\n",
        "\n",
        "    # поиск жилой площади и кухонной среди всех значений\n",
        "    if len(characteristics) < 3:\n",
        "        first_value = 0\n",
        "        second_value = 0\n",
        "        third_value = 0\n",
        "    else:\n",
        "        first_value = float(characteristics[0])\n",
        "        second_value = float(characteristics[1])\n",
        "        third_value = float(characteristics[2])\n",
        "\n",
        "        if first_value > second_value + third_value:\n",
        "            living_meters = second_value\n",
        "            kitchen_meters = third_value\n",
        "        else:\n",
        "            living_meters = second_value\n",
        "            kitchen_meters = 0\n",
        "\n",
        "        # создание новой строки\n",
        "        new_row = [url_value, str(living_meters), str(kitchen_meters), str(y_c)]\n",
        "\n",
        "        b += 1\n",
        "        print(b)\n",
        "\n",
        "        return new_row\n",
        "\n",
        "\n",
        "data = pd.read_csv('b_data.csv')\n",
        "urls = data['url'].tolist()[:]\n",
        "b = 0\n",
        "\n",
        "\n",
        "# Сбор данных в 6 параллельных потоков\n",
        "err_counter = 0\n",
        "CONNECTIONS = 6  # кол-во потоков\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=CONNECTIONS) as executor:\n",
        "    future_to_url = (executor.submit(get_info_from_link, url) for url in urls)\n",
        "    for future in tqdm(concurrent.futures.as_completed(future_to_url), total=len(urls)):\n",
        "        try:\n",
        "            info = future.result()\n",
        "            add_row = True\n",
        "        except Exception as exc:\n",
        "            add_row = False\n",
        "            err_counter += 1\n",
        "            err.write(str(exc) + \"\\n\")\n",
        "        finally:\n",
        "            if add_row and info:\n",
        "                print(info)\n",
        "                rows_f.write(\",\".join(info) + \"\\n\")\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end - start) * 10 ** 3, \"ms\")\n",
        "\n",
        "rows_f.close()\n",
        "err.close()\n",
        "print(f\"Ошибок: {err_counter}\")\n",
        "print('Успехов:', success)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Добавление дополнительных характеристик в основной датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('info_.csv')\n",
        "df2 = pd.read_csv('b_data.csv')\n",
        "\n",
        "\n",
        "# Поочередное сравнение url и присваивание значений из df1 в df2\n",
        "for index, row in df1.iterrows():\n",
        "    url = row['url']\n",
        "    mask = df2['url'] == url\n",
        "\n",
        "    if mask.any():\n",
        "        df2.loc[mask, 'living_meters'] = row['living_meters']\n",
        "        df2.loc[mask, 'kitchen_meters'] = row['kitchen_meters']\n",
        "        df2.loc[mask, 'year_of_construction'] = row['year_of_construction']\n",
        "    \n",
        "\n",
        "df2.to_csv('b_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сбор стоимости квартир по ссылкам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Загрузка датасета\n",
        "data = pd.read_csv('b_data.csv')\n",
        "\n",
        "# Датасет для записи\n",
        "data1 = pd.DataFrame(columns=['url', 'price'])\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "price_classes = [\"a10a3f92e9--amount--v3ROW\", \"a10a3f92e9--amount--ON6i1\"]\n",
        "\n",
        "# Перебор ссылок\n",
        "for index, row in data.iterrows():\n",
        "    url = row['url']\n",
        "    for price_class in price_classes:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        price_element = soup.find(\"div\", class_=price_class)\n",
        "        \n",
        "        if price_element is not None:\n",
        "            price_text = price_element.text.strip()\n",
        "            price_text = int(''.join(filter(str.isdigit, price_text)))\n",
        "            \n",
        "            # Заполнение построчно\n",
        "            new_row = pd.DataFrame({'url': [url], 'price': [price_text]})\n",
        "            data1 = pd.concat([data1, new_row], ignore_index=True)\n",
        "            \n",
        "            print(f\"Ссылка: {url}, Цена: {price_text}\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Ссылка: {url}, Цена: не найдена\")\n",
        "        \n",
        "# Экспорт\n",
        "data1.to_csv('apr17.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Загрузка датасета\n",
        "data = pd.read_csv('b_data.csv')\n",
        "\n",
        "# Датасет для записи\n",
        "data1 = pd.DataFrame(columns=['url', 'price'])\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "price_classes = [\"a10a3f92e9--amount--v3ROW\", \"a10a3f92e9--amount--ON6i1\"]\n",
        "\n",
        "# Перебор ссылок\n",
        "for index, row in data.iterrows():\n",
        "    url = row['url']\n",
        "    for price_class in price_classes:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        price_element = soup.find(\"div\", class_=price_class)\n",
        "        \n",
        "        if price_element is not None:\n",
        "            price_text = price_element.text.strip()\n",
        "            price_text = int(''.join(filter(str.isdigit, price_text)))\n",
        "            \n",
        "            # Заполнение построчно\n",
        "            new_row = pd.DataFrame({'url': [url], 'price': [price_text]})\n",
        "            data1 = pd.concat([data1, new_row], ignore_index=True)\n",
        "            \n",
        "            print(f\"Ссылка: {url}, Цена: {price_text}\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Ссылка: {url}, Цена: не найдена\")\n",
        "        \n",
        "# Экспорт\n",
        "data1.to_csv('may25.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Добавление изменения цен в основной датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('b_data.csv')\n",
        "df2 = pd.read_csv('apr17.csv')\n",
        "df3 = pd.read_csv('may25.csv')\n",
        "\n",
        "# Переименовывание\n",
        "df1.rename(columns={'price': 'price_feb'}, inplace=True)\n",
        "df2.rename(columns={'price': 'price_apr'}, inplace=True)\n",
        "df3.rename(columns={'price': 'price_may'}, inplace=True)\n",
        "\n",
        "# Сравнение url и добавление значений\n",
        "for index, row in df1.iterrows():\n",
        "    url = row['url']\n",
        "    if url in df2['url'].values:\n",
        "        df1.loc[index, 'price_apr'] = df2.loc[df2['url'] == url, 'price_apr'].values[0]\n",
        "    \n",
        "    if url in df3['url'].values:\n",
        "        df1.loc[index, 'price_may'] = df3.loc[df3['url'] == url, 'price_may'].values[0]\n",
        "\n",
        "df1 = df1[['url', 'location', 'floor', 'floors_count', \n",
        "           'rooms_count', 'living_meters', 'kitchen_meters', 'total_meters','price_feb', 'price_apr', \n",
        "           'price_may', 'district', 'street',\n",
        "           'underground', 'residential_complex', 'year_of_construction']]\n",
        "\n",
        "# Сохранение измененного датасета\n",
        "df1.to_csv('b_data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
